DEPENDENCIES-
python3, pip modules specified in requirements.txt

INTRODUCTION-

This tool helps for comparing the ML Capability in intent recognition for the platforms Kore.ai, Luis.ai and API.ai.

PREPARATION OF DATA-
1. Mention the Intents and the train data to be created in the ML_Train.csv file. While entering data , please follow the format as it is in ML_Train.csv
2. Mention the test data along with utterance classification ( i.e Positive/Negative/Spell Errors etc) in ML_TestData.csv. Here it is important to give the classification names properly as mentioned in the ML_TestData.csv sheet, with the necessary capitalization.

HOW TO CONFIGURE-


In the configBot.py File, follow the mentioned steps to get all the necessary tokens and populate them.

Prerequisites:
    - python 3.5.3 or higher
        - windowns installer at : https://www.python.org/downloads/windows/
    - virtualenv
        - windows instructions at https://docs.python.org/3/library/venv.html

HOW TO RUN-
Use Python 3.5

For Easy use:
Linux: ./run.sh

More customisable use:

1. Create a virtual env for this tool, if not already present.
     # creates venv but if more than one python is installed use "$ virtualenv  --python=python3 venv/ "
     $ virtualenv  venv/ 
     # start using venv
     $ source venv/bin/activate          
2. If this is your first time running the tool, do this.
     $ pip install --upgrade pip
     $ (windows only) download twisted from https://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted (appropriate version)
     $ (windows only) pip install Twisted<version>.whl
     $ pip install --upgrade -r requirements.txt
3. (Do this only if you haven't edited configBot.py else skip this step) To create a bot, upload training data, and test it and store results all at once, do
     $ "python pipeline.py"
4. (If you edited configBot.py) Do these steps separately,
     $ python createIntent.py
     $ python read.py testconfig.json

Do not give spaces in bot name. It is not tested with spaces, in all platforms.
Note that kore platform access token expires in finite time. so if you plan to run the testing at a later time, please update the token in testconfig.json (not configBot.py). The testing script (read.py) will prompt if kore token expires.

Currently it is possible to configure the tool to enable or disable running against Kore, Luis and DialogFlow, via configBot.py

HOW TO EVALUATE THE RESULTS-

1. The Results sheet has the status pass or fail depending on the intent identified matched with expected intent.
2. The Summary sheet shows Precision, Recall, F Measure and Accuracy Values for all the three platforms.
3. The IndividualIntents sheet shows the True Positives, True Negatives, False Positives and False Negatives for all the three platforms for individual tasks.

NOTES- Certain limitations are to be noted:
	a. Maximum number of Intents that can be created in Luis.AI are 12.
	b. Maximum number of endpoint hits for Luis.AI are 1000 hits per month for an account(For all the bots in the account together)
	c. Maximum number of agents that can be created in API.AI are 15


Configuration for API.ai -:

First of all create a new bot(referred as agent in api.ai). Press on the gear icon on the left next to the bot name. Copy the Developer_Access_Token.

#In the URL of the page, copy the bot id next to 'editAgent/' .


#Configuration for Luis.ai -:

#As soon as you login into the Luis website, press on My Keys. In this page, copy the Programmatic API key and paste it within the keys in subscriptionToken.

#Configuration for Kore.ai -:

#Enter the name of the environment for which the Benchmark tool needs to be run.

KorePlatform="https://bots.kore.ai"


#Configuration for IBM Watson -:

  Create an account in Watson, and get developer name and password (different from login user name and password)
  Limitations: free account allows a maximum of 5 bots at a time. Also there is a rate limit for bot&intent creation&deletion.




Previously run results can be found at https://github.com/Koredotcom/NLP-Benchmark/blob/master/previous-test-results/ML_Results-13-12-2017--21-45-17.ods.xlsx

